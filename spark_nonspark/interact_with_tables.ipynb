{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da75135a",
   "metadata": {},
   "source": [
    "- Adapted from: https://docs.databricks.com/aws/en/dev-tools/databricks-connect/python/examples\n",
    "- Key items:\n",
    "    - Use U2M OAuth (previously set up by Databricks CLI and profile)\n",
    "    - Use serverless compute during spark session init\n",
    "    - Use unity catalog 3-level name space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac0c5f",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session to Remote Serverless Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd890913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "\n",
    "# init the spark session to connect to the workspace serverless compute:\n",
    "spark = DatabricksSession.builder.profile(\"joshuale-common\").serverless(True).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8695684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0x76714c104650>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can validate the session:\n",
    "DatabricksSession.builder.validateSession(True).profile(\"joshuale-common\").serverless(True).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bda771",
   "metadata": {},
   "source": [
    "## 2. Reading an Existing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970caa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|pickup_zip|dropoff_zip|\n",
      "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
      "| 2016-02-13 21:47:53|  2016-02-13 21:57:15|          1.4|        8.0|     10103|      10110|\n",
      "| 2016-02-13 18:29:09|  2016-02-13 18:37:23|         1.31|        7.5|     10023|      10023|\n",
      "| 2016-02-06 19:40:58|  2016-02-06 19:52:32|          1.8|        9.5|     10001|      10018|\n",
      "| 2016-02-12 19:06:43|  2016-02-12 19:20:54|          2.3|       11.5|     10044|      10111|\n",
      "| 2016-02-23 10:27:56|  2016-02-23 10:58:33|          2.6|       18.5|     10199|      10022|\n",
      "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Read a table from the Databricks catalog\n",
    "# This operation will be executed on the serverless compute as dictated by the spark session.\n",
    "df = spark.read.table(\"samples.nyctaxi.trips\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e332ef0e",
   "metadata": {},
   "source": [
    "## 3. Create, Add Data to, Query from a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5690128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, DateType, StructType, StructField\n",
    "from datetime import date\n",
    "\n",
    "catalog_name = \"workspace\"\n",
    "schema_name = \"bronze\"\n",
    "table_name = \"demo_temps_table\"\n",
    "# Create a Spark DataFrame consisting of high and low temperatures\n",
    "# by airport code and date.\n",
    "schema = StructType([\n",
    "  StructField('AirportCode', StringType(), False),\n",
    "  StructField('Date', DateType(), False),\n",
    "  StructField('TempHighF', IntegerType(), False),\n",
    "  StructField('TempLowF', IntegerType(), False)\n",
    "])\n",
    "\n",
    "data = [\n",
    "  [ 'BLI', date(2021, 4, 3), 52, 43],\n",
    "  [ 'BLI', date(2021, 4, 2), 50, 38],\n",
    "  [ 'BLI', date(2021, 4, 1), 52, 41],\n",
    "  [ 'PDX', date(2021, 4, 3), 64, 45],\n",
    "  [ 'PDX', date(2021, 4, 2), 61, 41],\n",
    "  [ 'PDX', date(2021, 4, 1), 66, 39],\n",
    "  [ 'SEA', date(2021, 4, 3), 57, 43],\n",
    "  [ 'SEA', date(2021, 4, 2), 54, 39],\n",
    "  [ 'SEA', date(2021, 4, 1), 56, 41]\n",
    "]\n",
    "\n",
    "temps = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd61772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|pickup_zip|dropoff_zip|\n",
      "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
      "| 2016-02-13 21:47:53|  2016-02-13 21:57:15|          1.4|        8.0|     10103|      10110|\n",
      "| 2016-02-13 18:29:09|  2016-02-13 18:37:23|         1.31|        7.5|     10023|      10023|\n",
      "| 2016-02-06 19:40:58|  2016-02-06 19:52:32|          1.8|        9.5|     10001|      10018|\n",
      "| 2016-02-12 19:06:43|  2016-02-12 19:20:54|          2.3|       11.5|     10044|      10111|\n",
      "| 2016-02-23 10:27:56|  2016-02-23 10:58:33|          2.6|       18.5|     10199|      10022|\n",
      "| 2016-02-13 00:41:43|  2016-02-13 00:46:52|          1.4|        6.5|     10023|      10069|\n",
      "| 2016-02-18 23:49:53|  2016-02-19 00:12:53|         10.4|       31.0|     11371|      10003|\n",
      "| 2016-02-18 20:21:45|  2016-02-18 20:38:23|        10.15|       28.5|     11371|      11201|\n",
      "| 2016-02-03 10:47:50|  2016-02-03 11:07:06|         3.27|       15.0|     10014|      10023|\n",
      "| 2016-02-19 01:26:39|  2016-02-19 01:40:01|         4.42|       15.0|     10003|      11222|\n",
      "| 2016-02-12 00:19:38|  2016-02-12 00:34:59|          3.5|       13.5|     10012|      10018|\n",
      "| 2016-02-18 07:32:18|  2016-02-18 07:37:16|          1.1|        6.0|     10009|      10110|\n",
      "| 2016-02-24 13:58:21|  2016-02-24 14:13:02|          1.1|       10.0|     10119|      10017|\n",
      "| 2016-02-29 11:36:24|  2016-02-29 11:47:16|         0.93|        8.0|     10065|      10167|\n",
      "| 2016-02-12 15:55:09|  2016-02-12 16:05:16|         1.63|        9.0|     10021|      10023|\n",
      "| 2016-02-11 18:39:25|  2016-02-11 18:44:26|         0.71|        5.5|     10021|      10065|\n",
      "| 2016-02-20 22:09:04|  2016-02-20 22:22:43|          2.0|       10.5|     10001|      10003|\n",
      "| 2016-02-26 16:31:44|  2016-02-26 16:46:00|          1.1|       10.0|     10153|      10110|\n",
      "| 2016-02-26 09:24:38|  2016-02-26 09:33:49|          1.3|        8.5|     10025|      10029|\n",
      "| 2016-02-08 23:16:27|  2016-02-08 23:32:43|          3.4|       14.0|     10018|      10028|\n",
      "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table on the Databricks cluster and then fill\n",
    "# the table with the DataFrame's contents.\n",
    "# If the table already exists from a previous run,\n",
    "# delete it first.\n",
    "spark.sql(f'USE {catalog_name}.{schema_name}')\n",
    "spark.sql(f'DROP TABLE IF EXISTS {catalog_name}.{schema_name}.{table_name}')\n",
    "temps.write.saveAsTable(f'{catalog_name}.{schema_name}.{table_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5894732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+\n",
      "|AirportCode|      Date|TempHighF|TempLowF|\n",
      "+-----------+----------+---------+--------+\n",
      "|        PDX|2021-04-03|       64|      45|\n",
      "|        PDX|2021-04-02|       61|      41|\n",
      "|        SEA|2021-04-03|       57|      43|\n",
      "|        SEA|2021-04-02|       54|      39|\n",
      "+-----------+----------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query the table on the Databricks cluster, returning rows\n",
    "# where the airport code is not BLI and the date is later\n",
    "# than 2021-04-01. Group the results and order by high\n",
    "# temperature in descending order.\n",
    "df_temps = spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.{table_name} \" \\\n",
    "  \"WHERE AirportCode != 'BLI' AND Date > '2021-04-01' \" \\\n",
    "  \"GROUP BY AirportCode, Date, TempHighF, TempLowF \" \\\n",
    "  \"ORDER BY TempHighF DESC\")\n",
    "df_temps.show()\n",
    "\n",
    "# Results:\n",
    "#\n",
    "# +-----------+----------+---------+--------+\n",
    "# |AirportCode|      Date|TempHighF|TempLowF|\n",
    "# +-----------+----------+---------+--------+\n",
    "# |        PDX|2021-04-03|       64|      45|\n",
    "# |        PDX|2021-04-02|       61|      41|\n",
    "# |        SEA|2021-04-03|       57|      43|\n",
    "# |        SEA|2021-04-02|       54|      39|\n",
    "# +-----------+----------+---------+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfbd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up by deleting the table from the Databricks cluster.\n",
    "spark.sql(f'DROP TABLE IF EXISTS {catalog_name}.{schema_name}.{table_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databricks-connect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
